{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "inp = pd.read_csv('Input/Cleaned_besk_eform.csv', dtype=object, encoding='iso-8859-1', names=['besk'], skiprows=[0]) \n",
    "\n",
    "nace=inp[\"besk\"].str.split(\";\", n=1, expand=True)\n",
    "\n",
    "nace=nace[nace[0].notnull()]\n",
    "nace=shuffle(nace)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "besk=nace[0].values\n",
    "sn07=nace[1].values\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "y_encode = lab_enc.fit_transform(sn07)\n",
    "\n",
    "import keras\n",
    "\n",
    "y = keras.utils.to_categorical(y_encode, 841)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(besk)\n",
    "X=t.texts_to_sequences(besk)\n",
    "\n",
    "vocab_size=len(t.word_index)+1\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen=162\n",
    "\n",
    "X_pad=pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.5, random_state=1000)\n",
    "\n",
    "#normalizer = preprocessing.Normalizer().fit(X_train)\n",
    "#X_train_norm=normalizer.transform(X_train)\n",
    "#X_test_norm=normalizer.transform(X_test)\n",
    "\n",
    "#x_train= tf.keras.utils.normalize(X_train, axis=1)\n",
    "#x_test= tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "\n",
    "#longest_string_sn07=max(sn07, key=len)\n",
    "#len(longest_string_sn07)\n",
    "#longest_string_besk=max(besk, key=len)\n",
    "#len(longest_string_besk)\n",
    "#max(y_encode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-dense-1-conv-64-layer-1558011182\n",
      "Train on 120709 samples, validate on 120709 samples\n",
      "Epoch 1/10\n",
      "120709/120709 [==============================] - 869s 7ms/step - loss: 4.3734 - acc: 0.1483 - val_loss: 3.6565 - val_acc: 0.2636\n",
      "Epoch 2/10\n",
      "120709/120709 [==============================] - 414s 3ms/step - loss: 3.4048 - acc: 0.2902 - val_loss: 3.2114 - val_acc: 0.3462\n",
      "Epoch 3/10\n",
      "120709/120709 [==============================] - 428s 4ms/step - loss: 2.9708 - acc: 0.3593 - val_loss: 3.0367 - val_acc: 0.3886\n",
      "Epoch 4/10\n",
      "120709/120709 [==============================] - 435s 4ms/step - loss: 2.6953 - acc: 0.4046 - val_loss: 2.9679 - val_acc: 0.4134\n",
      "Epoch 5/10\n",
      "120709/120709 [==============================] - 419s 3ms/step - loss: 2.4927 - acc: 0.4407 - val_loss: 2.9460 - val_acc: 0.4283\n",
      "Epoch 6/10\n",
      "120709/120709 [==============================] - 527s 4ms/step - loss: 2.3344 - acc: 0.4695 - val_loss: 2.9634 - val_acc: 0.4371\n",
      "Epoch 7/10\n",
      "120709/120709 [==============================] - 404s 3ms/step - loss: 2.2145 - acc: 0.4918 - val_loss: 2.9834 - val_acc: 0.4433\n",
      "Epoch 8/10\n",
      "120709/120709 [==============================] - 472s 4ms/step - loss: 2.1018 - acc: 0.5140 - val_loss: 3.0200 - val_acc: 0.4478\n",
      "Epoch 9/10\n",
      " 19616/120709 [===>..........................] - ETA: 4:58 - loss: 2.0099 - acc: 0.5294"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, SeparableConv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import normalize\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "# learning rate schedule\n",
    "#def step_decay(epoch):\n",
    "#    initial_lrate = 0.001\n",
    "#    drop = 0.1\n",
    "#    epochs_drop = 2\n",
    "#    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "#    return lrate\n",
    "\n",
    "#input_dim = X_train.shape[1:]\n",
    "embedding_dim=50\n",
    "\n",
    "dense_layers=[0,1,2]\n",
    "conv_layers=[1,2,3]\n",
    "layer_sizes=[64,128,256]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME=\"{}-dense-{}-conv-{}-layer-{}\".format(dense_layer, conv_layer, layer_size, int(time.time()))\n",
    "            tensorboard=TensorBoard(log_dir='logs/6/{}'.format(NAME))\n",
    "            print(NAME)\n",
    "            model=Sequential()\n",
    "            model.add(layers.Embedding(input_dim=vocab_size,\n",
    "                                       output_dim=embedding_dim,\n",
    "                                       input_length=maxlen))\n",
    "            model.add(layers.SeparableConv1D(layer_size ,5, activation='relu'))\n",
    "            model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(layers.SeparableConv1D(layer_size ,5, activation='relu'))\n",
    "                model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "            model.add(layers.Flatten())\n",
    "            for l in range(dense_layer):\n",
    "                model.add(layers.Dense(layer_size, activation='relu'))\n",
    "                model.add(layers.Dropout(0.2))\n",
    "\n",
    "            model.add(layers.Dense(841, activation='softmax'))\n",
    "            epochs = 10\n",
    "            learning_rate = 0.001\n",
    "            decay_rate = learning_rate / epochs\n",
    "            adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "            #lrate = LearningRateScheduler(step_decay)\n",
    "            callbacks_list = [tensorboard]\n",
    "            model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "\n",
    "#Ã¥pne tensorboard: tensorboard --logdi=ml/logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
