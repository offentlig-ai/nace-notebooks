{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#RENSET DATASETT: STOPWORDS ETC:\n",
    "#inp = pd.read_csv('Input/Cleaned_besk_eform.csv', dtype=object, encoding='iso-8859-1', names=['besk'], skiprows=[0]) \n",
    "\n",
    "#nace=inp[\"besk\"].str.split(\";\", n=1, expand=True)\n",
    "\n",
    "#nace=nace[nace[0].notnull()]\n",
    "#nace=shuffle(nace)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#besk=nace[0].values\n",
    "#sn07=nace[1].values\n",
    "###############################\n",
    "\n",
    "#URENSET DATASETT:\n",
    "nace = pd.read_csv('Input/BESK_ALLE.csv', sep=';', encoding='iso-8859-1') \n",
    "\n",
    "nace=nace[nace[\"besk\"].notnull()]\n",
    "nace=shuffle(nace)\n",
    "\n",
    "\"\"\"Split into training and test set\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "besk=nace[\"besk\"].values\n",
    "sn07=nace[\"SN07_1\"].values\n",
    "###############################\n",
    "\n",
    "#PRETRAINED VECTOR\n",
    "import io\n",
    "import numpy as np\n",
    "file = io.open('Input/cc.no.300.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "vocab_and_vectors = {}\n",
    "# put words as dict indexes and vectors as words values\n",
    "for line in file:\n",
    "  values = line.split()\n",
    "  word = values [0]\n",
    "  vector = np.asarray(values[1:], dtype='float32')\n",
    "  vocab_and_vectors[word] = vector\n",
    "###############################\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "y_encode = lab_enc.fit_transform(sn07)\n",
    "\n",
    "import keras\n",
    "\n",
    "y = keras.utils.to_categorical(y_encode, 841)\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(besk)\n",
    "X=t.texts_to_sequences(besk)\n",
    "\n",
    "vocab_size=len(t.word_index)+1\n",
    "\n",
    "word_index = t.word_index\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen=162\n",
    "\n",
    "X_pad=pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.5, random_state=1000)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "  embedding_vector = vocab_and_vectors.get(word)\n",
    "  # words that cannot be found will be set to 0\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "#normalizer = preprocessing.Normalizer().fit(X_train)\n",
    "#X_train_norm=normalizer.transform(X_train)\n",
    "#X_test_norm=normalizer.transform(X_test)\n",
    "\n",
    "#x_train= tf.keras.utils.normalize(X_train, axis=1)\n",
    "#x_test= tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "\n",
    "#longest_string_sn07=max(sn07, key=len)\n",
    "#len(longest_string_sn07)\n",
    "#longest_string_besk=max(besk, key=len)\n",
    "#len(longest_string_besk)\n",
    "#max(y_encode)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 790937 samples, validate on 790938 samples\n",
      "Epoch 1/10\n",
      "790937/790937 [==============================] - 16588s 21ms/step - loss: 2.1527 - acc: 0.5474 - val_loss: 1.8115 - val_acc: 0.5971\n",
      "Epoch 2/10\n",
      "790937/790937 [==============================] - 16558s 21ms/step - loss: 1.5774 - acc: 0.6336 - val_loss: 1.7559 - val_acc: 0.6059\n",
      "Epoch 3/10\n",
      "790937/790937 [==============================] - 16549s 21ms/step - loss: 1.4019 - acc: 0.6684 - val_loss: 1.7676 - val_acc: 0.6069\n",
      "Epoch 4/10\n",
      "790937/790937 [==============================] - 16666s 21ms/step - loss: 1.2942 - acc: 0.6923 - val_loss: 1.7876 - val_acc: 0.6057\n",
      "Epoch 5/10\n",
      "790937/790937 [==============================] - 16626s 21ms/step - loss: 1.2182 - acc: 0.7099 - val_loss: 1.8090 - val_acc: 0.6043\n",
      "Epoch 6/10\n",
      "790937/790937 [==============================] - 17696s 22ms/step - loss: 1.1615 - acc: 0.7232 - val_loss: 1.8311 - val_acc: 0.6033\n",
      "Epoch 7/10\n",
      "790937/790937 [==============================] - 16675s 21ms/step - loss: 1.1177 - acc: 0.7334 - val_loss: 1.8584 - val_acc: 0.6018\n",
      "Epoch 8/10\n",
      "790937/790937 [==============================] - 16678s 21ms/step - loss: 1.0813 - acc: 0.7422 - val_loss: 1.8760 - val_acc: 0.6008\n",
      "Epoch 9/10\n",
      "790937/790937 [==============================] - 16695s 21ms/step - loss: 1.0530 - acc: 0.7487 - val_loss: 1.8949 - val_acc: 0.5997\n",
      "Epoch 10/10\n",
      "790937/790937 [==============================] - 16689s 21ms/step - loss: 1.0278 - acc: 0.7546 - val_loss: 1.9106 - val_acc: 0.5987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2d2b82240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, SeparableConv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import normalize\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "# learning rate schedule\n",
    "#def step_decay(epoch):\n",
    "#    initial_lrate = 0.001\n",
    "#    drop = 0.1\n",
    "#    epochs_drop = 2\n",
    "#    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "#    return lrate\n",
    "\n",
    "#input_dim = X_train.shape[1:]\n",
    "embedding_dim=300\n",
    "\n",
    "#NAME=\"{}-dense-{}-conv-{}-layer-{}\".format(dense_layer, conv_layer, layer_size, int(time.time()))\n",
    "#tensorboard=TensorBoard(log_dir='logs/7/{}'.format(NAME))\n",
    "#print(NAME)\n",
    "model=Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embedding_dim,\n",
    "                           input_length=maxlen, \n",
    "                           weights=[embedding_matrix],\n",
    "                           trainable=True))\n",
    "model.add(layers.SeparableConv1D(256 ,5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "            \n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(841, activation='softmax'))\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "decay_rate = learning_rate / epochs\n",
    "adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#lrate = LearningRateScheduler(step_decay)\n",
    "#callbacks_list = [tensorboard]\n",
    "model.fit(X_train, y_train,\n",
    "        epochs=10,\n",
    "        validation_data=(X_test, y_test))#, callbacks=callbacks_list)\n",
    "\n",
    "#åpne tensorboard: tensorboard --logdi=ml/logs/\n",
    "#Accuracy på et utvalg fra datasettet: 52% for renset og urenset sett\n",
    "#Accuracy for hele settet: 60% for renset og urenset sett"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
